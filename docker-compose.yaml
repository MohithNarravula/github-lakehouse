version: '3.8'

services:
  # --- SPARK CLUSTER ---
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_MASTER_HOST=spark-master
    volumes:
      - ./data:/opt/spark/data
    networks:
      - spark-net

  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
    volumes:
      - ./data:/opt/spark/data
    networks:
      - spark-net

  jupyter:
    build: .
    container_name: jupyter
    depends_on:
      - spark-master
    ports:
      - "8888:8888"
    environment:
      - PYSPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./src:/opt/spark/src
      - ./conf:/opt/spark/conf
      - ./data:/opt/spark/data
      - ./notebooks:/opt/spark/notebooks
      - ./requirements.txt:/opt/spark/requirements.txt
      - ./.env:/opt/spark/.env
      - ./dags:/opt/airflow/dags # Shared DAGs folder
    networks:
      - spark-net

  # --- AIRFLOW SECTION ---
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - spark-net

  airflow-webserver:
    image: apache/airflow:2.7.1-python3.10
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    ports:
      - "8085:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/spark/src
      - ./data:/opt/spark/data
      - ./.env:/opt/airflow/.env
    entrypoint: /bin/bash -c "airflow db init && airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com && airflow webserver"
    networks:
      - spark-net

  airflow-scheduler:
    image: apache/airflow:2.7.1-python3.10
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/spark/src
      - ./data:/opt/spark/data
      - ./.env:/opt/airflow/.env
    command: scheduler
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge